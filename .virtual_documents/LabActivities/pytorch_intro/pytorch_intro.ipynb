














import torch
import numpy as np
import matplotlib.pyplot as plt

def f(x, y):
    return torch.sqrt(x**2 + y**2)
    
x = torch.tensor(3.0)
y = torch.tensor(4.0)

print(f(x, y))





x = torch.tensor(3.0, requires_grad=True)
y = torch.tensor(4.0, requires_grad=True)

fxy = f(x, y) # Evaluate the function.

fxy.backward() # Calculate the partial derivatives.

print(f"f(3, 4) = {fxy:.5}")
print(f"df(3, 4)/dx = {x.grad:.5}")
print(f"df(3, 4)/dy = {y.grad:.5}")





learning_rate = .01  # ADJUST THIS!
iterations = 10      # AND/OR THIS!

x = torch.tensor(3.0, requires_grad=True)
y = torch.tensor(4.0, requires_grad=True)

losses = []

for iteration in range(iterations):
    fxy = f(x, y)

    losses.append(fxy.item())
    
    # One step of gradient descent...
    fxy.backward()
    x.data = x.data - learning_rate * x.grad
    y.data = y.data - learning_rate * y.grad
    
    # By default, the gradients will continue to accumulate.
    # We need to zero it out each iteration to get a fresh result.
    x.grad.zero_()
    y.grad.zero_()

plt.plot(losses, '.-')
plt.xlabel("step")
plt.ylabel("loss")
plt.show()
print("\nFinal estimate: ")
fxy = f(x, y)
print(f"f({x:.5}, {y:.5}) = {fxy:.5}")

    











import matplotlib.pyplot as plt
# %matplotlib notebook
x = np.linspace(-2, 2, 100)
y = np.linspace(-2, 2, 100)
x, y = np.meshgrid(x, y)
z = np.sin(5 * x + 3) + x**2 + np.cos(2 * y + 1) + y**2
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
surf = ax.plot_surface(x, y, z, cmap=plt.cm.coolwarm,
                       linewidth=0, antialiased=False)
ax.set_xlabel('x')
ax.set_ylabel('y')
plt.show()



# YOUR CODE HERE
raise NotImplementedError()


from numpy.testing import assert_almost_equal
assert_almost_equal(x.data, .317, decimal=3)
assert_almost_equal(y.data, .690, decimal=3)
