{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bb4399d4c6a5a37e40fbd0146bdb363e",
     "grade": false,
     "grade_id": "cell-018d75d41635244c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Model Selection and Evaluation - Part 1\n",
    "\n",
    "When building supervised machine learning models, we need to solve two\n",
    "problems:\n",
    "\n",
    "1. **Model selection** - Finding the model that does as well as possible\n",
    "on our learning task.\n",
    "\n",
    "2. **Model evaluation** - Predicting **generalization error**, or the expected performance of our\n",
    "model on unseen data.\n",
    "\n",
    "Both are critical.  Without 1. we can't have an effective model and\n",
    "without 2. we can't *know* if we have an effective model.\n",
    "\n",
    "## Parameters and Hyperparameters\n",
    "\n",
    "Building a machine learning model involves both **parameters** and **hyperparameters**:\n",
    "\n",
    "* The **parameters** of a model are learned directly from the training data. For example, in the case of fitting a polynomial, the parameters are the \"learned\" polynomial coefficients of the best fit.\n",
    "* **Hyperparameters** are parameters of our learning models that need to be selected before the model can be learned.  In the example of fitting a polynomial, the key hyperparameter is the degree of the polynomial.  \n",
    "\n",
    "Once we have picked a particular machine learning algorithm, model\n",
    "selection comes down to the problem of **hyperparameter** tuning.\n",
    "\n",
    "**WARNING:**  In the exercises below will showcase several *BAD* approaches to model selection and evaluation.  These examples are not meant to illustrate the correct way of doing things, they are meant to show the consequences of doing things incorrectly. \n",
    "\n",
    "**ENTER YOUR NAMES IN THE CELL BELOW**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dc7ac7a1b3b40de7873711d34e89eb77",
     "grade": true,
     "grade_id": "Names",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "SAYEMUM HASSAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dbd3dced12c19926949746a2ef842409",
     "grade": false,
     "grade_id": "cell-9423187334a01aa9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "----\n",
    "## Questions:\n",
    "* Describe the *parameters* of the decision tree learning algorithm.\n",
    "* Describe some *hyperparameters* of the decision tree learning algorithm. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ff3e6183b40a3642bc8168a7d8148fbc",
     "grade": true,
     "grade_id": "tree_params_and_hyperparams",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Parameters - node splits, leaf nodes ---------\n",
    "Hyperparameters - max_leaf_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ebb388510f73409e488aabbbeb3ae84f",
     "grade": false,
     "grade_id": "cell-b672d3849c3303a5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 1 - Naive Model Selection\n",
    "\n",
    "For now, let's focus entirely on model selection and disregard model\n",
    "evaluation.  The following cell will load a data set and use a\n",
    "decision tree regressor to fit a decision tree to the data. Try adjusting the `max_leaf_nodes` hyperparameter in order to minimize the MSE on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib qt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sprinkle_data\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Grab our training data\n",
    "source = sprinkle_data.SprinkleDataSource()\n",
    "X, y = source.gen_data(num=100, seed=100)\n",
    "\n",
    "# Build a decision tree regressor\n",
    "tree = DecisionTreeRegressor(max_leaf_nodes=3) # ADJUST THIS VALUE!\n",
    "tree.fit(X, y)\n",
    "\n",
    "# Evaluate the MSE of our decision tree on the training set\n",
    "y_predict = tree.predict(X)\n",
    "mse = np.sum((y - y_predict)**2) / y.size\n",
    "print(\"MSE: {:.4f}\".format(mse))\n",
    "\n",
    "# Plot the fit.\n",
    "plt.plot(X, y, '*')\n",
    "x_plt = np.linspace(0, 1, 400).reshape(400, 1)\n",
    "plt.plot(x_plt, tree.predict(x_plt))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8764f1ffa62a9d699bb09f2b59169ed4",
     "grade": true,
     "grade_id": "ex1_minimize_mse",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Write some code to find the best number of leaves..\n",
    "# You could write a loop here to loop over possible values for\n",
    "# max_leaf_nodes, and use matplotlib to plot the corresponding \n",
    "# MSE values. Note that the minimum value for max_leaf_nodes is 2.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "99831869705be8950b4c339b89ed277b",
     "grade": false,
     "grade_id": "cell-729c4e07aacd739a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Questions\n",
    "\n",
    "* What value of the hyperparameter resulted in the lowest MSE?\n",
    "* Do you think that this MSE reflects how well this model will do on unseen data?  Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0fa1b59e8542f133b381edbd176dc093",
     "grade": true,
     "grade_id": "ex1_discuss_mse",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "afe3907c5239bd96884da3f6c0d83059",
     "grade": false,
     "grade_id": "cell-6409755393625ed2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the exercise above, you were able to tune the hyperparameters so as to *perfectly* fit the training data.  Now let's see what happens when we use this model on some new data drawn from the same underlying distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeRegressor(max_leaf_nodes=??) # Fill in the optimal answer you found above.\n",
    "tree.fit(X, y)\n",
    "\n",
    "# Generate some new data from the same distribution:\n",
    "X_new, y_new = source.gen_data(100000, seed=200)\n",
    "\n",
    "# Evaluate our model on the new data:\n",
    "y_new_predict = tree.predict(X_new)\n",
    "mse = np.sum((y_new - y_new_predict)**2) / y_new.size\n",
    "print(\"MSE: {:.4f}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "476b84fa8e19e976c3faea51c856b48c",
     "grade": false,
     "grade_id": "cell-85ebab4af05d4bb8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 2 - Using a Test Set for Hyperparameter Tuning and Evaluation\n",
    "\n",
    "In the exercise above, you were able to perfectly fit a training data set, but that didn't tell you anything about how well your model would perform on unseen data. Recall that we want our models to **generalize**, that is, perform well on data that the model has not seen\n",
    "previously.\n",
    "\n",
    "We might address this by splitting our limited data into a **training set** and a **test set**.  The training set is used to fit the parameters, and the test set is used as a proxy for unseen data. This is illustrated in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our data into a training and testing set...\n",
    "split_point = int(X.shape[0] * .8) # Use 80% of the data to train the model\n",
    "\n",
    "X_train = X[:split_point, :]\n",
    "y_train = y[:split_point]\n",
    "\n",
    "X_test = X[split_point:, :]\n",
    "y_test = y[split_point:]\n",
    "\n",
    "# Build a decision tree regressor using the TRAINING set\n",
    "tree = DecisionTreeRegressor(max_leaf_nodes=3)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the MSE of our decision tree on the TESTING set \n",
    "y_test_predict = tree.predict(X_test)\n",
    "mse = np.sum((y_test - y_test_predict)**2) / y_test.size\n",
    "print(\"MSE: {:.4f}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "84f9c9ce2cd7d4d850bf67af84423dbe",
     "grade": false,
     "grade_id": "cell-d51c2bc59688edae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the code block below, write a loop that iterates over the hyperparameter for the number of leaves and reports the value that produces the lowest error rate on the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f212b29b0d4d1bc3fedb590ca4d8a64",
     "grade": false,
     "grade_id": "cell-7061424cade65017",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def explore_num_leaves(X_train, y_train, X_test, y_test, max_max_leaves):\n",
    "    \"\"\" Systematically evaluate different settings for max_leaf_nodes by\n",
    "    building a decision tree at each possible size and evaluating MSE\n",
    "    on both the training set the test set.\n",
    "    \n",
    "    (Note that the minimum value for the max_leaf_nodes argument to \n",
    "    DecisionTreeRegressor is 2.)\n",
    "    \n",
    "    Returns:\n",
    "       train_mse - Numpy array of length max_max_leaves - 1. Entry zero \n",
    "                   corresponds MSE for a tree with two leaves, entry one corresponds \n",
    "                   to three leaves etc.\n",
    "       test_mse -  Same structure, but containing MSE on the test set. \n",
    "    \n",
    "    \"\"\"\n",
    "    train_mse = np.zeros((max_max_leaves-1,))\n",
    "    test_mse = np.zeros((max_max_leaves-1,))\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return train_mse, test_mse\n",
    "\n",
    "# Run the experiment\n",
    "max_max_leaves = 80\n",
    "train_mse, test_mse = explore_num_leaves(X_train, y_train, X_test, y_test, max_max_leaves)\n",
    "\n",
    "# Plot the result\n",
    "num_leaves = np.arange(2, max_max_leaves + 1)\n",
    "plt.plot(num_leaves, train_mse, '.-', label='MSE train')\n",
    "plt.plot(num_leaves, test_mse, '.-', label='MSE test')\n",
    "plt.xlabel('num leaves')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95b67d4a83634ae4eab66221caf4fa0f",
     "grade": true,
     "grade_id": "exercise-2-test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#TEST ON FIRST AND LAST VALUES\n",
    "np.testing.assert_almost_equal(train_mse[0], 0.35245863397873406)\n",
    "np.testing.assert_almost_equal(test_mse[0], 0.2300883386121208)\n",
    "np.testing.assert_almost_equal(train_mse[-1], 0)\n",
    "np.testing.assert_almost_equal(test_mse[-1], 0.16938288794527132)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "240a9b174c1dfd7b2f8c47929d32aa63",
     "grade": false,
     "grade_id": "cell-e8274baca9aaa860",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Questions\n",
    "* What hyperparameter settings gives us the lowest MSE on the testing data?  What is the MSE? \n",
    "* Do you think *this* MSE will be reflective of how well our model will perform on unseen data? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f841e17d538991d5e500fbc72e007d56",
     "grade": true,
     "grade_id": "ex2_best_hyperparams_w_training",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "260d57c7b4ab8ccee081c021659968c4",
     "grade": false,
     "grade_id": "cell-d5509f5cbcbba52d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Checking Performance on Unseen Data\n",
    "\n",
    "Notice that in this example we are using our test set for *both* model selection and model evaluation.  We used it for model selection by searching for a hyperparameter setting that minimizes error on the test set.  We use it for model evaluation by using our test set error as an estimate of the expected error rate on unobserved data.\n",
    "\n",
    "Let's see how our model does on some new, unobserved data drawn from the same distribution.  This cell will give us a good estimate of our *actual* generalization error.  (Note that in real-world problems we can't run a test like this because we don't have unlimited access to extra data that we can use to check our work.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeRegressor(max_leaf_nodes=??) # Put your best hyperparameter here!\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Let's see how we do on unobserved data... \n",
    "X_new, y_new = source.gen_data(100000, seed=200)\n",
    "y_new_predict = tree.predict(X_new)\n",
    "mse = np.sum((y_new - y_new_predict)**2) / y_new.size\n",
    "print(\"MSE: {:.4f}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "af6fe01cd767a1cb717546c080fa8fb0",
     "grade": false,
     "grade_id": "cell-d16448544b059b57",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Questions\n",
    "\n",
    "* Relative to Exercise 1, where we just looked for the model that best fit our training data, would you say that our train/test split was beneficial in terms of *model selection*, i.e. did the test set help us to find a model with lower prediction error on unseen data? Justify your answer.\n",
    "* Would you say that our train/test split was beneficial in terms of *model evaluation*, i.e. was the error rate on the test set a good predictor of the error rate on unseen data?  Did we overestimate the error rate or underestimate it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b39f3e4b55ebc6713923342823ffd2d1",
     "grade": true,
     "grade_id": "ex2_review",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Click [here](model_selection_2.ipynb) to open the next page of exercises..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
