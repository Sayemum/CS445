{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KAmR4c1feAWd"
   },
   "source": [
    "# Recurrent Neural Networks In PyTorch\n",
    "\n",
    "Learning Objectives:\n",
    "\n",
    "* Improve understanding of the basic structure of recurrent neural networks.\n",
    "* Gain experience working with simple recurrent neural networks in PyTorch. \n",
    "\n",
    "In this activity we will consider the problem of making real-valued predictions from one-dimensional sequence data.  For example, we might want to make a series of temperature readings every hour and then predict the temperature for the next hour from the previous 24.  The cell below contains a Python function for creating a synthetic data set of numerical sequences.\n",
    "\n",
    "(If you are running this notebook on Google Colab, make sure to select GPU acceleration under `Edit->Notebook Settings`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "resources": []
    },
    "colab_type": "code",
    "id": "uudeKcljUlyS"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def sin_dataset(num, length, noise =.01):\n",
    "    \"\"\" Return a sequnce dataset composed of many short snippets of noisy\n",
    "        sin functions.  The snippets all have randomly generated periods \n",
    "        and offsets.\n",
    "\n",
    "        Arguments:\n",
    "           num - number of sequences to return\n",
    "           length - length of the sequences\n",
    "           noise - std of normal noise added to each point\n",
    "\n",
    "        Returns: x, y \n",
    "                Where x is a numpy array with shape \n",
    "                (num, lengh, 1), and y is an array with shape (length,)\n",
    "                The values in y are the (noisy) next values for the \n",
    "                corresponding sequences in x\n",
    "    \"\"\"\n",
    "\n",
    "    x = np.zeros((num, length, 1))\n",
    "    y = np.zeros(num)\n",
    "    \n",
    "    delta_t = (2 * np.pi) / 30\n",
    "    ins = np.arange(0, (length+1)*delta_t, delta_t)\n",
    "    for i in range(num):\n",
    "        data = np.sin(ins * (1.0 + np.random.random() * 4.0) + \n",
    "                      np.random.random() * 2.0 * np.pi)\n",
    "        \n",
    "        offset = np.random.random() * 10 - 5\n",
    "        scale = .1\n",
    "        data = (data + offset) * scale + np.random.randn(*data.shape) * noise\n",
    "        x[i, :, 0] = data[:-1]\n",
    "        y[i] = data[-1]\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q-nNvXynf6gB"
   },
   "source": [
    "## Training Set\n",
    "\n",
    "Execute the cell below to generate a training data set and visualize some of the input sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282,
     "resources": []
    },
    "colab_type": "code",
    "id": "flHBzeiMUZhm",
    "outputId": "907dd062-8522-4afd-9716-6439a07728c1"
   },
   "outputs": [],
   "source": [
    "seq_length = 30\n",
    "num_seqs = 200000\n",
    "    \n",
    "sequences, targets = sin_dataset(num_seqs, seq_length)\n",
    "print(targets.shape)\n",
    "\n",
    "plt.plot(sequences[0:10, : ,0].T, '.-')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loaders\n",
    "\n",
    "Execute the cell below to create torch DataLoader objects to use in our training loop below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def set_device(new_device=None):\n",
    "    global device\n",
    "    if new_device is None:\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda\"\n",
    "        elif torch.backends.mps.is_available():\n",
    "            device = \"mps\"\n",
    "        else:\n",
    "            device = \"cpu\"\n",
    "    else:\n",
    "        device=new_device\n",
    "        \n",
    "set_device()\n",
    "print(\"Using:\", device)\n",
    "\n",
    "batch_size = 64\n",
    "tensor_x = torch.Tensor(sequences)\n",
    "tensor_y = torch.Tensor(np.expand_dims(targets, axis=1))\n",
    "\n",
    "dataset = TensorDataset(tensor_x, tensor_y)\n",
    "\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [.9, .1])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
    "                                           shuffle=True, num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I_Eog_5zgl-N"
   },
   "source": [
    "## Exploring Network Structure\n",
    "\n",
    "The cell below constructs an *extremely* simple RNN for the prediction task described above.  As you will see when you execute the cell, this network has exactly 13 weights. (This is actually a bit of a glitch in the torch RNN implementation.  The count should be 11, but torch gives each hidden unit two bias weights.)\n",
    "\n",
    "### Questions\n",
    "* On a separate sheet of paper, draw the structure of the network created in the cell below.  Every weight in the network should have a corresponding arrow in your diagram.  There should be exactly 11 arrows.\n",
    "* Also on a separate sheet of paper, draw the \"unrolled\" version of this network for three time steps.   If drawn correctly, there should be exactly 33 arrows in your diagram (with four arrows pointing to the nonexistent hidden units from the fourth time step.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225,
     "resources": []
    },
    "colab_type": "code",
    "id": "j8-gyngBHtSU",
    "outputId": "d92b3726-e7de-4277-d672-704b37e4d0a7"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    \n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.rnn = nn.RNN(input_size=1, hidden_size=2, num_layers=1, batch_first=True)\n",
    "    self.out = nn.Linear(2, 1)\n",
    "\n",
    "  def forward(self,x):\n",
    "    x,_ = self.rnn(x)  # Second output is the final hidden state, which we don't need.\n",
    "    x = self.out(x[:, -1, :])  # Output is the activation at the end of the sequence.\n",
    "    return x\n",
    "\n",
    "model = SimpleRNN()\n",
    "\n",
    "print(model)\n",
    "print(\"Num parameters: \", count_parameters(model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZsXyDOK9qKe8"
   },
   "source": [
    "## Training\n",
    "\n",
    "The cell below contains code for fitting a torch model.  Go ahead and train the model now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312,
     "resources": []
    },
    "colab_type": "code",
    "id": "CKL4nlkghOi0",
    "outputId": "890eadd0-81c4-4904-e593-2bd080906168"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()  # Set the \"training\" flag to true for the model.\n",
    "    total_loss = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "\n",
    "    print(f\"Epoch time: {time.time() - start_time:.4f}(s)\")\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "    test_loss /= num_batches\n",
    "\n",
    "    return test_loss\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, loss_fn, optimizer, epochs):\n",
    "    writer = SummaryWriter()\n",
    "    for t in range(epochs):\n",
    "        train_one_epoch(train_loader, model, loss_fn, optimizer)\n",
    "        train_loss = test(train_loader, model, loss_fn)\n",
    "        val_loss = test(val_loader, model, loss_fn)\n",
    "\n",
    "        train_str = f\"loss: {train_loss:.6f}  \"\n",
    "        val_str = f\"validation loss: {val_loss:.6f}\"\n",
    "\n",
    "        writer.add_scalar('Loss/train', train_loss, t)\n",
    "        writer.add_scalar('Loss/val', val_loss, t)\n",
    "\n",
    "        print(f\"Epoch {t+1} \" + train_str + val_str)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=.001)\n",
    "\n",
    "set_device()\n",
    "model = model.to(device)\n",
    "\n",
    "train(model, train_loader, val_loader, loss_fn, optimizer, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uPBT1IciqkLj"
   },
   "source": [
    "## TensorBoard\n",
    "\n",
    "The cell below will start up tensorboard and allow you to examine the loss values.  If you train multiple times you can refresh the page using the circular arrow on the upper-right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 839,
     "resources": []
    },
    "colab_type": "code",
    "id": "g9yGJBR8mLuy",
    "outputId": "b6bb3200-b764-4670-a68a-3731c2ff2a02"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S-1xgpWOre0b"
   },
   "source": [
    "## Examining The Results\n",
    "\n",
    "The cell below will allow us to visulize the predictions made by our model.  First, we predict the single next value in the sequence, then we append that value to our sequence and use the resulting sequence to predict the next value.  This way we can make predictions arbitrarily far into the future.  Unfortunately, our predictions are likely to get farther and farther off as small initial errors compound.  You can re-run the cell multiple times to see what happens with different initialial sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265,
     "resources": []
    },
    "colab_type": "code",
    "id": "AKaS5LmAT9em",
    "outputId": "7e346a37-f7b9-4851-beb8-c84cdf4f9b5b"
   },
   "outputs": [],
   "source": [
    "# For some reason GPU was crashing on lab machines here.  \n",
    "# Let's move the model back to the cpu before testing...\n",
    "set_device(\"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "num_predict = 50\n",
    "\n",
    "test_input, test_y = sin_dataset(1, seq_length + num_predict)\n",
    "cur_input = torch.Tensor(test_input[:, 0:seq_length, :])\n",
    "predictions = []\n",
    "indices = []\n",
    "for i in range(num_predict):\n",
    "    indices.append(i + seq_length)\n",
    "    y = model.forward(cur_input)\n",
    "    cur_input = torch.cat((cur_input[:, 1:, :], y.unsqueeze(1)), axis=1)\n",
    "    predictions.append(y.detach().numpy().flatten())\n",
    "\n",
    "plt.plot(test_input[0, :, 0], '.-')\n",
    "plt.plot(indices, predictions, '.-')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BxcuNR-8sxf0"
   },
   "source": [
    "## Improving the Model\n",
    "\n",
    "There are many things we could try that might improve the performance of our current model:\n",
    "* Add more hidden units to our RNN\n",
    "* Try fancier RNN units like LSTM or GRU\n",
    "* Add more RNN layers\n",
    "\n",
    "\n",
    "### Activity\n",
    "\n",
    "Experiment with improving performance on the description task above.  How low can you get the validation error?  Can you significantly improve the quality of the predictions over multiple time steps? "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "rnn_exercises.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
